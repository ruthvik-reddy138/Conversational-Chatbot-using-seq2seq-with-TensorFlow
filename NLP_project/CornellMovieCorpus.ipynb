{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Attention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from convokit import Corpus, download\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell Movie-Dialogs Corpus is now being loaded from existing data...\n"
     ]
    }
   ],
   "source": [
    "# Prepare a directory to store the data\n",
    "data_directory = \"data\"\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# Assign a path for storing the movie corpus\n",
    "movie_corpus_path = os.path.join(data_directory, \"movie-corpus\")\n",
    "\n",
    "# Download the Cornell Movie-Dialogs Corpus if it's not already present\n",
    "if not os.path.exists(movie_corpus_path):\n",
    "    print(\"Commencing download of the Cornell Movie-Dialogs Corpus...\")\n",
    "    dialog_corpus = Corpus(filename=download(\"movie-corpus\", data_dir=data_directory))\n",
    "else:\n",
    "    print(\"Cornell Movie-Dialogs Corpus is now being loaded from existing data...\")\n",
    "    dialog_corpus = Corpus(filename=movie_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dialog pairs extracted: 1000\n"
     ]
    }
   ],
   "source": [
    "# Extract dialog pairs\n",
    "query_list, response_list = [], []\n",
    "max_samples = 1000\n",
    "\n",
    "for conv_id in dialog_corpus.conversations:\n",
    "    if len(query_list) >= max_samples:\n",
    "        break\n",
    "    conversation = dialog_corpus.get_conversation(conv_id)\n",
    "    utterance_ids = conversation.get_utterance_ids()\n",
    "    for i in range(len(utterance_ids) - 1):\n",
    "        if len(query_list) >= max_samples:\n",
    "            break\n",
    "        query_list.append(dialog_corpus.get_utterance(utterance_ids[i]).text)\n",
    "        response_list.append(dialog_corpus.get_utterance(utterance_ids[i + 1]).text)\n",
    "\n",
    "print(f\"Total dialog pairs extracted: {len(query_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "dialog_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "dialog_tokenizer.fit_on_texts(query_list + response_list)\n",
    "word_count = len(dialog_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dialogs into sequences\n",
    "seq_queries = dialog_tokenizer.texts_to_sequences(query_list)\n",
    "seq_responses = dialog_tokenizer.texts_to_sequences(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length for padding\n",
    "sequence_max_length = max(max(len(item) for item in seq_queries), max(len(item) for item in seq_responses))\n",
    "seq_queries_padded = pad_sequences(seq_queries, maxlen=sequence_max_length, padding='post')\n",
    "seq_responses_padded = pad_sequences(seq_responses, maxlen=sequence_max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Seq2Seq model with Attention\n",
    "embedding_dimension = 64\n",
    "lstm_unit_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(sequence_max_length,))\n",
    "encoder_embedding = Embedding(word_count, embedding_dimension, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(lstm_unit_size, return_state=True, return_sequences=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(sequence_max_length,))\n",
    "decoder_embedding = Embedding(word_count, embedding_dimension, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_unit_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "attention_layer = Attention()\n",
    "attention_result = attention_layer([decoder_outputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Attention output and Decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "decoder_dense = Dense(word_count, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 23:42:38.597955: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 12s 144ms/step - loss: 6.7989 - accuracy: 0.1075\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 9s 143ms/step - loss: 5.8997 - accuracy: 0.1120\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 10s 151ms/step - loss: 5.7086 - accuracy: 0.1120\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 9s 151ms/step - loss: 5.5796 - accuracy: 0.1123\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 9s 144ms/step - loss: 5.4818 - accuracy: 0.1120\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 9s 147ms/step - loss: 5.4046 - accuracy: 0.1118\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 9s 146ms/step - loss: 5.3288 - accuracy: 0.1121\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 9s 149ms/step - loss: 5.2521 - accuracy: 0.1129\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 9s 146ms/step - loss: 5.1842 - accuracy: 0.1146\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 9s 149ms/step - loss: 5.1044 - accuracy: 0.1196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3ad685870>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "decoder_target_data = np.zeros(seq_responses_padded.shape)\n",
    "decoder_target_data[:,:-1] = seq_responses_padded[:,1:]\n",
    "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
    "\n",
    "model.fit([seq_queries_padded, seq_responses_padded], decoder_target_data, batch_size=16, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Other variants like \"gpt2-medium\" or \"gpt2-large\" can be used for better performance\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of question templates\n",
    "question_templates = [\n",
    "    \"How is Mission Impossible?\",\n",
    "    \"How can I [action] [object]?\",\n",
    "    \"Tell me more about [subject].\",\n",
    "    \"What is the [adjective] way to [action] [object]?\",\n",
    "    \"Can you explain [concept] to me?\",\n",
    "    \"What are the benefits of [action]?\",\n",
    "    \"Give me some information on [topic].\",\n",
    "    \"What are your thoughts on [topic]?\",\n",
    "    \"How does [process] work?\",\n",
    "    \"Recommend a [adjective] [object] for me.\",\n",
    "]\n",
    "\n",
    "# List of placeholders to fill in the question templates\n",
    "placeholders = {\n",
    "    \"[action]\": [\"learn\", \"understand\", \"use\", \"improve\", \"explore\"],\n",
    "    \"[object]\": [\"Python programming\", \"machine learning\", \"data analysis\", \"web development\", \"chatbots\"],\n",
    "    \"[subject]\": [\"your favorite book\", \"the latest technology\", \"the weather in Paris\", \"your hobbies\"],\n",
    "    \"[adjective]\": [\"best\", \"easiest\", \"fastest\", \"most popular\", \"recommended\"],\n",
    "    \"[concept]\": [\"artificial intelligence\", \"blockchain\", \"quantum computing\", \"neural networks\"],\n",
    "    \"[topic]\": [\"space exploration\", \"environmental sustainability\", \"the stock market\", \"travel destinations\"],\n",
    "    \"[process]\": [\"photosynthesis\", \"protein synthesis\", \"cloud computing\", \"data encryption\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a random user input question\n",
    "def generate_random_user_input():\n",
    "    question_template = random.choice(question_templates)\n",
    "    for placeholder, options in placeholders.items():\n",
    "        replacement = random.choice(options)\n",
    "        question_template = question_template.replace(placeholder, replacement)\n",
    "    return question_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# Function to generate chatbot responses using GPT-2 with varied responses\n",
    "def generate_chatbot_responses(user_input, num_responses=1, max_length=50):\n",
    "    responses = []\n",
    "    set_seed(random.randint(1, 10000))  # Setting a random seed for variability in responses\n",
    "    for _ in range(num_responses):\n",
    "        inputs = tokenizer.encode(user_input, return_tensors='pt')\n",
    "        outputs = gpt2_model.generate(\n",
    "            inputs, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,  # Adjusts randomness in response generation\n",
    "            top_k=50,        # Limits the number of highest probability vocabulary tokens to consider for each step\n",
    "            top_p=0.95,      # Nucleus sampling: chooses from the smallest possible set of tokens whose cumulative probability exceeds the threshold p\n",
    "            no_repeat_ngram_size=2  # Prevents repeating n-grams in generated text\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input Question: How is Mission Impossible?\n",
      "Chatbot Responses:\n",
      "Response 1: How is Mission Impossible?\n",
      "\n",
      "Mission Impossible is a film that is about a group of people who are trying to save the world from a mysterious alien invasion. The film is set in the year 2055, and the film's plot revolves around a\n"
     ]
    }
   ],
   "source": [
    "user_input_question = generate_random_user_input()\n",
    "chatbot_generated_responses = generate_chatbot_responses(user_input_question)\n",
    "\n",
    "print(\"User Input Question:\", user_input_question)\n",
    "print(\"Chatbot Responses:\")\n",
    "for i, response in enumerate(chatbot_generated_responses):\n",
    "    print(f\"Response {i + 1}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
